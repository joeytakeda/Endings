Products (draw on our previous work on HTML5/JS/CSS, and our relentless validation stuff; also discuss the work on Mariage that tested products using the Heritrage harvester)


[Note: This assumes the re-organization of our Principles document as discussed before, to create version 2.0.]

4.1 No dependence on server-side software: build a static website with no databases, no PHP, no Python. 

"Lots of Copies Keep Stuff Safe," says the conventional wisdom, and Stanford has an entire digital preservation program (https://www.lockss.org/) named for this truism. And in fact there is no doubt that the more copies of something exist, the more likely it is to survive over the long term, especially if those copies are widely distributed geographically [citation needed -- see ?https://brewminate.com/classics-lost-and-found-the-survival-of-ancient-texts/]. In the world of online digital resources, though, it doesn't really matter how many archival copies of your TEI XML source encoding are distributed across the world; if the one hosting server which runs the database backend and WordPress front-end for your digital edition goes down, to all intents and purposes your project has disappeared.

We have therefore focused on building digital editions which can run on any web server, anywhere, without any specific dependencies. If spinning up a new site requires nothing more than copying a collection of files to a server and circulating the URL, there is much more chance that _functional_ copies of the _products_ of your work will survive in a usable form. Every server-side dependency is a barrier to replication and therefore to survival.

4.2 No boutique or fashionable technologies: use only standards with support across all platforms, whose long-term viability is assured. Our choices are HTML5, JavaScript and CSS. 

4.3 No dependence on external libraries or services: no JQuery, no AngularJS, no Bootstrap, no Google Search.

The World Wide Web -- the front-end of the modern Internet -- is perhaps the most successful and prolific invention in the history of human communication. Its current scale dwarfs the entire prior history of text, and it continues to expand at an astonishing rate. Underlying its functionality are three core languages: the markup language HTML, the style language CSS, and the scripting language JavaScript (ECMAScript), all of which are well-managed by standards bodies. This trio of technologies underlies more than the web, of course; cell phone applications, ePub documents, and many other forms of communication are also built on the base technologies of the Web, all of which are relatively simple and easy even for beginners to understand and learn.

However, the world of content-creators who build Web resources is not so simple; rarely do they sit down and write plain HTML documents and style them with CSS. Instead, they tend to use large packaged libraries of existing code to generate the end-user Web pages we consume. Large coding frameworks for creating web-based resources come and go at a remarkable speed. At the time of writing, Angular, React, Vue, Express.js, Meteor, and Node are all popular JavaScript frameworks, while JQuery, Dojo, MooTools and others have fallen out of favour; by the time you read this, the situation will certainly have changed again. The same is true of database-dependent content-creation and delivery tools such as WordPress, Ruby on Rails, Drupal, Joomla and others, as well as all of the tempting "free" services that have enabled DH practitioners to create intriguing mash-ups and to decorate their work with data from other sources. All of these efforts promise rapid site development at the cost of long-term maintenance issues. As Jim Nielsen (https://blog.jim-nielsen.com/2021/web-languages-as-compile-targets/) and others have pointed out, this creates an ecosystem in which "all web languages—HTML, CSS, and JS—are compile targets". Programmers no longer code directly in the target languages, and many do not even know them very well. Instead, they become specialists in a few fashionable frameworks, and chase the changing fashions year by year.

And yet, the trifecta of those three core languages is a remarkably powerful. We know that because ultimately, all of those other languages, frameworks and tools, from MySQL+PHP to PostGresql+Python to RethinkDB+Node.js, basically do one thing: they produce HTML/CSS/JavaScript Web pages, and those Web pages do all the things we need them to do. And while each of those technologies or frameworks or back-end services will eventually stop working, for one reason or another, it is extremely unlikely that Web pages themselves will cease to function. One of the most remarkable things about HTML, CSS, and JavaScript is that over 20+ years of development, they have retained remarkable levels of backward compatibility. The first Web pages ever created still work perfectly (http://info.cern.ch/hypertext/WWW/TheProject.html); as we discussed in the introduction, the first projects we created at HCMC in the 1990s and early 2000s are also still perfectly functional. It is not unreasonable to think that, given the astonishing quantity of resources built and delivered through HTML, CSS, and JavaScript, there is a strong chance that they will continue to function over the long term; and when they do, perhaps, alter in ways that make older forms of text less usable, there will be readily-available migration pathways and tools that can bring along everything that survives and is worth preserving. The same will not be true of this year's favourite JavaScript framework or last year's most popular content management system. 

In the same way, if maintenance is ever required, it is very likely that there will be plenty of programmers familiar with the three core languages, able to read and understand standard code. It is not so certain that programmers capable of debugging and fixing a ReactJS application or a PHP script will be so common. Building your products from the three core technologies substantially increases their chances of survival; dependence on back-end or external services is a temporary solution for short-term projects.

4.4 No query strings: every entity in the site has a unique page with a simple URL that will function on any domain or ip address.

As noted above, older sites which queried back-end databases to construct their "pages" tended to have page URLs whose distinctive factors lay in the query string rather than in the base location. For example, to retrieve the diary entry for 22 February 1935 from the Robert Graves Diary project required the following URL:

http://graves.uvic.ca/graves/site/xbrowse.xq?collection=%2Fdb%2Fgraves&type=diaryentry&query_stored=false&action=browse&search_text=-1&day=22&month=02&year=1935

This concoction presented obvious difficulties for the Internet Archive crawler, so none of the diary entries were actually archived in the Wayback Machine from the original site. Contrast this with the current equivalent:

http://graves.uvic.ca/diary_1935-02-22.html

(See Holmes 2017 ["Selecting Technologies for Long-Term Survival." (Conference presentation.) SHARP Conference 2017: Technologies of the Book, Victoria, BC, Canada. [https://github.com/projectEndings/Endings/raw/master/presentations/SHARP_2017/mdh_sharp_2017.pdf]. 10/06/2017. ])

which itself is linked from a sitemap page which ensures its retrieval. Such URLs are not only cleaner and simpler for automated tools to process; they can also be hacked by humans who happen to know exactly what date (in this case) they are looking for, and do not need to engage with form controls or other GUI components to get where they need to go.

In addition to ensuring that every URL is simple and meaningful, we also believe that every entity (document, prosopography entry, bibliography entry, collaborator biography) should have its own unique page with its own URL. This provides maximum flexibility in terms of linking, extraction of individual pages, and re-use by future researchers (see below). Many metadata schemes (such as RDF) rely on URLs as unique identifiers for categories, concepts and entities; a scholarly publication can embody this approach, and support Linked Open Data applications more effectively, by segmenting entities at the page level in the site structure.

4.5 Massive redundancy: every page contains all the components it needs, so that it will function without the rest of the site if necessary, even though this means duplicating information across the site.

If we consider the life a static website surviving (as we hope) untended and unremediated for decades into the future, it is difficult to imagine what its future audience might be, and in what ways they will be interacting with it. While one strategy for preservation is to place full copies of a site in as many places as possible, we should also consider that re-use and re-purposing of material is an important component of survival, as well as a measure of the value and interest the scholarly work may hold for future readers. However, re-use is unlikely to constitute re-use of the entire collection of documents in a project; more frequently, we may expect future users to take and re-purpose small subsets of the collection, even individual pages or documents, which are of particular use to them. 

Since we want to encourage this, it makes sense to provide documents in a form which makes it trivially easy to excise them from the collection. Documents should, as far as is practical, stand alone. Rather than having a document link to thirty external prosopography entries that live in other documents on the site, why not simply incorporate copies of all those entries inside the document itself? Text is typically small and compressable compared with some other digital resources. If we make a point of replicating all required linked records such as bibliography, gazetteer and personography entries inside each individual document, any user can take a single page and use it elsewhere without having to resolve a large number of pointers and retrieve a set of external resources to make the document functional. This of course means that the same individual personography entries may be replicated hundreds of times across a collection. This seems wasteful; but as we have said previously ("The Prefabricated Website: Who needs a server anyway?" (Conference presentation.) With Joey Takeda. Text Encoding Initiative Conference, Graz, Austria. [https://zenodo.org/record/3449197]. 19/09/2019. ), "We don't care." 

4.6 Graceful failure: every page should still basically work even in the absence of JavaScript or CSS support.

The concept of graceful failure (or degradation) of internet services has been discussed and recommended since the 1990s (see for example Jakob Nielsen [https://www.nngroup.com/articles/graceful-degradation-of-scalable-internet-services/]). However, while widely acknowledged, it is nowadays rarely implemented in a practical way. This is partly because modern websites have become so dependent on JavaScript and CSS that there is no way for them to function as plain HTML (imagine Google Docs or Office 365 without scripting); also, since so many sites now depend entirely on JavaScript and CSS, it has become impractical for ordinary users to turn off these features of their web browsers, with the result that no-one ever does, and graceful degradation is never needed or tested in the real world.

However, static websites lend themselves rather well to graceful degradation. If a page already contains all the secondary or supplementary information it requires in the form of biographies, bibliographical data, and so on, as we recommend above, then it can easily function effectively in the form of plain HTML, just like the earliest web pages of the 1990s. Through progressive enhancement, JavaScript may turn links to footnotes into buttons that generate convenient popups, but the underlying page structure can be plain and functional even in the absence of CSS and JavaScript. This makes validation and link-checking trivial to implement, which is important for the next facet.

[DRAFTED TO HERE]

4.7 Relentless validation: every site build involves validation of all input data (XML) and all output code (HTML5, JavaScript, CSS).

4.8 Inclusion of data: every site should include a documented copy of the source data, so that users of the site can repurpose the work easily.

These principles are tempered by the following concessions:

4.9 Once a fully-working static site is achieved, it may be enhanced by the use of other services such as a server-side indexing tool (Solr, eXist) to support searching and similar functionality.

4.10 The use of an external library may be necessary to support a specific function which is too complex to be coded locally (such as mapping or cryptography). Any such libraries must be open-source and widely-used, and must not themselves have dependencies. 




